{"cells":[{"cell_type":"markdown","metadata":{"id":"v0yRQ_ZSMaFG"},"source":["# **Import packages and functions**"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6068,"status":"ok","timestamp":1669394123216,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"},"user_tz":-60},"id":"uwBik9AUO-VZ"},"outputs":[],"source":["# functions and packages\n","import pandas as pd\n","import numpy as np\n","import os\n","import pickle\n","import random\n","from scipy.stats import spearmanr\n","from sklearn.metrics import roc_auc_score\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from pandas.api.types import is_string_dtype\n","from collections import OrderedDict\n","import matplotlib.pyplot as plt\n","plt.style.use('fivethirtyeight')\n","from scipy.spatial import distance\n","\n","#LSTM\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Dense, Dropout, Input, Multiply, concatenate, Embedding, LSTM\n","from tensorflow.keras.layers import Bidirectional, TimeDistributed\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Nadam, Adam, SGD, RMSprop\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","import tensorflow.keras.utils as ku\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.layers import Softmax, Lambda\n","from tensorflow.keras import backend\n","\n","#CNN\n","from tensorflow.keras.layers import Conv1D\n","\n","#packages from https://github.com/irhete/predictive-monitoring-benchmark/blob/master/experiments/experiments.py\n","from data_preparation import EncoderFactory\n","from data_preparation.DatasetManager import DatasetManager\n","# from dataset_manager import DatasetManager\n","\n","from sklearn import metrics"]},{"cell_type":"markdown","metadata":{"id":"iSXBB6FWOyRN"},"source":["## **Own created functions**"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1669394123217,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"},"user_tz":-60},"id":"uH3LRIBHOyap"},"outputs":[],"source":["#functions\n","#https://towardsdatascience.com/using-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2\n","class ColumnEncoder(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        self.columns = None\n","        self.maps = dict()\n","\n","    def transform(self, X):\n","        X_copy = X.copy()\n","        for col in self.columns:\n","            # encode value x of col via dict entry self.maps[col][x]+1 if present, otherwise 0\n","            X_copy.loc[:,col] = X_copy.loc[:,col].apply(lambda x: self.maps[col].get(x, -1)+1)\n","        return X_copy\n","\n","    def inverse_transform(self, X):\n","        X_copy = X.copy()\n","        for col in self.columns:\n","            values = list(self.maps[col].keys())\n","            # find value in ordered list and map out of range values to None\n","            X_copy.loc[:,col] = [values[i-1] if 0<i<=len(values) else None for i in X_copy[col]]\n","        return X_copy\n","\n","    def fit(self, X, y=None):\n","        # only apply to string type columns\n","        self.columns = [col for col in X.columns if is_string_dtype(X[col])]\n","        for col in self.columns:\n","            self.maps[col] = OrderedDict({value: num for num, value in enumerate(sorted(set(X[col])))})\n","        return self\n","\n","def prepare_inputs(X_train, X_test, data):  \n","    global ce\n","    ce = ColumnEncoder()\n","    X_train, X_test = X_train.astype(str), X_test.astype(str)\n","    X_train_enc = ce.fit_transform(X_train)\n","    X_test_enc = ce.transform(X_test)\n","    return X_train_enc, X_test_enc\n","    \n","def numeric_padding(sequences, maxlen=None, value=0):\n","    num_samples = len(sequences)\n","    sample_shape = np.asarray(sequences[0]).shape[1:]\n","    x = np.full((num_samples, maxlen) + sample_shape, value)\n","    for idx, s in enumerate(sequences):\n","        trunc = s[:maxlen]\n","        x[idx, :maxlen] = trunc[0]\n","        \n","def create_index(log_df, column):\n","    \"\"\"Creates an idx for a categorical attribute.\n","    Args:\n","        log_df: dataframe.\n","        column: column name.\n","    Returns:\n","        index of a categorical attribute pairs.\n","    \"\"\"\n","    temp_list = log_df[[column]].values.tolist()\n","    subsec_set = {str((x[0])) for x in temp_list}\n","    subsec_set = sorted(list(subsec_set))\n","    alias = dict()\n","    for i, _ in enumerate(subsec_set):\n","        alias[subsec_set[i]] = i + 1\n","    return alias\n","\n","def groupby_caseID(data, cols):\n","    ans = [pd.DataFrame(y) for x, y in data[cols].groupby('Case ID', as_index=False)]\n","    return ans\n","\n","def remove_punctuations(columns_before):\n","    columns = []\n","    for string in columns_before:\n","        new_string = string.replace(\":\", \"_\")\n","        columns.append(new_string)\n","    return columns\n","\n","#call this function with the name of the right column\n","def create_indexes(i, data):\n","    cat_index = create_index(data, i)\n","    cat_index['Start'] = 0\n","    cat_index['End'] = len(cat_index)\n","    index_cat = {v: k for k, v in cat_index.items()}\n","    cat_weights = ku.to_categorical(sorted(index_cat.keys()), len(cat_index))\n","    no_cols = len(data.groupby([i]))+1\n","    return cat_weights, index_cat, cat_index, no_cols\n","\n","def labels_after_grouping(data_train,data_test):\n","    train_labels = []\n","    for i in range (0,len(data_train)):\n","        temp_label = data_train[i]['label'].iloc[0]\n","        train_labels.append(temp_label)\n","\n","    test_labels = []\n","    for i in range (0,len(data_test)):\n","        temp_label = data_test[i]['label'].iloc[0]\n","        test_labels.append(temp_label)\n","    train_y = [1 if i!='regular' else 0 for i in train_labels]\n","    test_y = [1 if i!='regular' else 0 for i in test_labels]\n","    return train_y, test_y\n","\n","def pad_cat_data(cols, data_train, data_test, maxlen):\n","    \n","    #padding of the different categorical columns\n","    #train paddings\n","    paddings_train = []\n","    for i in cols:\n","        padding= []\n","        for k in range(0,len(data_train)):\n","            temp = []\n","            temp = list(data_train[k][i])\n","            padding.append(temp)\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        #padded = padded/len(data.groupby([i]))\n","        paddings_train.append(padded)\n","\n","    #test paddings\n","    paddings_test = []\n","    for i in cols:\n","        padding= []\n","        for k in range(0,len(data_test)):\n","            temp = []\n","            temp = list(data_test[k][i])\n","            padding.append(temp)\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        #padded = padded/len(data.groupby([i]))\n","        paddings_test.append(padded)\n","    return paddings_train, paddings_test\n","\n","def pad_num_data(cols, data_train, data_test, maxlen, dt_train_prefixes, dt_test_prefixes):\n","    pad_train = []\n","    pad_test  = []\n","    for i in cols:\n","        \n","        padding = []\n","        for k in range(0,len(data_train)):\n","            temp_train = []\n","            temp_train = list(data_train[k][i])\n","            padding.append(temp_train)\n","\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        if dt_train_prefixes[i].max() !=0:\n","           \n","            padded = padded/dt_train_prefixes[i].max()\n","        else:\n","            padded = padded\n","        pad_train.append(padded)\n","   \n","    for i in cols:\n","      \n","        padding = []\n","        for k in range(0,len(data_test)):\n","            temp_test = []\n","            temp_test = list(data_test[k][i])\n","            padding.append(temp_test)\n","      \n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        if dt_test_prefixes[i].max() !=0:\n","            padded = padded/dt_test_prefixes[i].max()\n","        else:\n","            padded = padded\n","        pad_test.append(padded)\n","    return pad_train, pad_test\n","\n","def reshape_num_data(pad_data, cutoff):\n","        pad_num = np.reshape(pad_data, (len(pad_data), cutoff, 1))\n","        return pad_num"]},{"cell_type":"markdown","metadata":{"id":"i00WpG-jxewE"},"source":["# **Attention**"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1669394130986,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"},"user_tz":-60},"id":"pI-3_ZdzxiEV"},"outputs":[],"source":["def attention():\n","    #  Generation of predictions\n","    layer_names = [layer.name for layer in model.layers]\n","    print(layer_names)\n","    ac_output_weights, ac_bias = model.get_layer(name='final_output').get_weights()\n","    model_with_attention = Model(model.inputs, model.outputs +\\\n","                                                  [model.get_layer(name='alpha_softmax').output,\\\n","                                                   model.get_layer(name='beta_dense_0').output])\n","    temporal_vectors = []\n","    variable_vectors=[]\n","    predictions = []\n","    for i in range(len(paddings_train[0])):\n","        x_ngram_list = []\n","        for j in range(0,len(paddings_train)):\n","                x_ngram = paddings_train[j][i].reshape((1,cutoff))\n","                x_ngram_list.append(x_ngram)\n","\n","        for k in range(0,len(pad_train)):\n","                x_ngram = np.reshape(pad_train[k], (len(pad_train[k]), cutoff, 1))[i].reshape(1,cutoff,1)\n","                x_ngram_list.append(x_ngram)\n","\n","        #extend list and add time \n","        x_ngram_list.append(padded_time[i].reshape(1,cutoff,1))\n","\n","        proba, alphas, betas = model_with_attention.predict(x_ngram_list)\n","        proba = np.squeeze(proba)\n","        alphas = np.squeeze(alphas)\n","        temporal_att_vec = alphas\n","        assert (np.sum(temporal_att_vec) - 1.0) < 1e-5\n","\n","        #print(temporal_att_vec)\n","        temporal_vectors.append(temporal_att_vec)\n","\n","        #get the beta value\n","        betas = np.squeeze(betas)\n","        idx = np.argmax(alphas)\n","\n","        #print(idx)\n","        beta_val = betas[idx]\n","\n","        dim = 0\n","        emb_list = []\n","        for i in range(0,len(paddings_train)):\n","            ip = int(x_ngram_list[i][0][idx])\n","            i = cat_cols[i]\n","            i = i.replace(':','_')\n","            i = i.replace(' ','_')\n","            emb_weights = model.get_layer(name='embed_'+i).get_weights()[0]\n","            emb = emb_weights[ip]\n","            emb_list.append(emb)\n","            dim += emb.shape[0]\n","\n","\n","        for i in range(0,len(pad_train)):\n","            x_ngram = (reshape_num_data(pad_train[k], cutoff))[i].reshape(1,cutoff,1)\n","            num = np.squeeze(x_ngram)[idx]\n","            emb_list.append(num)\n","            dim +=1\n","        #if cls_method =='LSTM':\n","        if(betas.shape[1]==dim+1):\n","            x_t_ngram = padded_time[i].reshape(1, cutoff, 1)\n","            time_v = np.squeeze(x_t_ngram)[idx]\n","            emb_list.append(time_v)\n","            emb = np.concatenate(tuple(emb_list), axis=None)\n","            #print('beta_val',beta_val.shape)\n","            beta_scaled = np.multiply(beta_val,emb)\n","            variable_attn = alphas[idx] * beta_scaled\n","            variable_vectors.append(variable_attn)\n","            predictions.append(proba)\n","       \n","    if(len(variable_vectors)>0):\n","        var_final = np.mean(np.array(variable_vectors), axis=0)\n","        cat_labels_list = []\n","        current_length = 0\n","        for i in dt_train_prefixes[cat_cols].columns:\n","            cat_weights, index_cat, cat_index, no_cols = create_indexes(i, data)\n","            cat_labels = [index_cat[key] for key in sorted(index_cat.keys())]\n","            cat_labels_list.extend(cat_labels)\n","\n","        for i in dt_train_prefixes[numerical_columns].columns:\n","            cat_labels_list.append(i)\n","        cat_labels_list.append('time')\n","        \n","        df_var=pd.DataFrame({'attributes':var_final, 'attribute_values':cat_labels_list})\n","        print(df_var)\n","        df_var.plot.bar(y='attributes', x='attribute_values',\n","                                    title='Attention of the event attributes.', figsize=(10,7))\n","\n","        #plot_history( plt, file_name + 'variable_attn', path )\n","\n","        # Hide grid lines\n","        plt.grid(False)\n","        plt.show()\n","        attention_values = []\n","        for i in dt_train_prefixes[cat_cols].columns:\n","            cat_weights, index_cat, cat_index, no_cols = create_indexes(i, data)\n","            new_length = len([index_cat[key] for key in sorted(index_cat.keys())])\n","            attention_value = abs(df_var['attributes'].iloc[current_length:current_length+new_length]).sum(skipna = True)\n","            attention_values.append(attention_value)\n","            current_length += new_length\n","      \n","        for i in dt_train_prefixes[numerical_columns].columns:\n","            attention_values.append(abs(df_var['attributes'].iloc[current_length:current_length+1]).sum(skipna = True))\n","            current_length+1\n","\n","        attention_values.append(abs(df_var['attributes'].iloc[current_length:current_length+1]).sum(skipna = True))\n","\n","    return df_var, attention_values"]},{"cell_type":"markdown","metadata":{"id":"t6qDWGGTxptx"},"source":["# **Parsimony**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HhodjI2f_s5F"},"outputs":[],"source":["def var_importance(model, model_inputs, train_y, sample_length):\n","    effects_saved = []\n","    x = data_sample(model_inputs, sample_length)\n","    orig_pred = model.predict(x)\n","    orig_out = metrics.mean_squared_error(train_y, orig_pred)\n","    for i in range(0,len(cat_cols)):  # iterate over the 5 cat features\n","        print(\" \")\n","        new_x = x.copy()\n","        perturbations_list = []\n","        for j in range(0,sample_length):\n","            prefix_array = np.random.random((maxlen,))\n","            perturbations_list.append(prefix_array)\n","        new_x[i] = np.array(perturbations_list)\n","        perturbed_out = model.predict(new_x)\n","        effect = ((orig_out - perturbed_out) ** 2).mean() ** 0.5\n","        print(f'Variable {i+1}, perturbation effect: {effect:.4f}')\n","        effects_saved.append(effect)\n","    for k in range(i+1,len(model_inputs)):  # iterate over the 2 num features\n","        print(k)\n","        new_x = x.copy()\n","        perturbations_list = []\n","        for j in range(0,sample_length):\n","            prefix_array = np.random.random((maxlen,))\n","            perturbations_list.append(prefix_array)\n","        perturbation_array = np.array(perturbations_list)\n","        perturbations_reshaped = perturbation_array.reshape(sample_length, cutoff, 1)\n","        new_x[k] = perturbations_reshaped\n","        perturbed_pred = model.predict(new_x)\n","        perturbed_out = metrics.mean_squared_error(train_y, perturbed_pred)\n","        effect = perturbed_out - orig_out\n","        print('Variable: ',j, 'perturbation effect: ',effect)   \n","        effects_saved.append(effect)\n","    return effects_saved \n","\n","def parsimony(attention_values):\n","  #feature importance of original model\n","  feature_importance=pd.DataFrame()\n","  columns = cat_cols+numerical_columns\n","  columns.append('time')\n","  feature_importance['variable']=columns\n","  feature_importance['coefficients'] = attention_values\n","  \n","  count_event = 0\n","  count_case = 0\n","  count_control = 0\n","\n","  # event columns\n","  model_event = feature_importance[feature_importance['variable'].isin(event_columns)]\n","  model_event = model_event['coefficients'].tolist()\n","  #case columns\n","  model_case= feature_importance[feature_importance['variable'].isin(case_columns)]\n","  model_case = model_case['coefficients'].tolist()\n","  count_case += model_case.count(0.0)\n","  #controlflow columns\n","  model_control= feature_importance[feature_importance['variable'].isin(controlflow_columns)]\n","  model_control = model_control['coefficients'].tolist()\n","  count_control += model_control.count(0.0)\n","  parsimony_event = (len_event-count_event)\n","  parsimony_case = (len_case-count_case)\n","  parsimony_control = (len_control-count_control)\n","  print('parsimony event attributes:', parsimony_event)\n","  print('parsimony case attributes:', parsimony_case)\n","  print('parsimony controlflow attributes', parsimony_control)\n","\n","  return parsimony_event, parsimony_case, parsimony_control\n"]},{"cell_type":"markdown","metadata":{"id":"3m4jrc3axxDj"},"source":["# **Functional complexity**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDT1w0aNYH7R"},"outputs":[],"source":["def create_test_data(dt_test_prefixes):\n","\n","  #cat columns integerencoded\n","  cat_cols = cls_encoder_args['dynamic_cat_cols']+cls_encoder_args['static_cat_cols']\n","  dt_test_prefixes[cat_cols] = dt_test_prefixes[cat_cols].astype(str)\n","  dt_test_prefixes[cat_cols] = ce.transform(dt_test_prefixes[cat_cols])\n","  dt_test_prefixes[cat_cols] = dt_test_prefixes[cat_cols]+1\n","  \n","  #append caseId and label\n","  cat_cols.append('Case ID')\n","  cat_cols.append('label')\n","  #groupby case ID\n","  ans_test = groupby_caseID(dt_test_prefixes, cat_cols)\n","\n","  #remove then back\n","  cat_cols.remove('label')\n","  cat_cols.remove('Case ID')\n","  \n","  #pad cat columns\n","  paddings_test = []\n","  for i in cat_cols:\n","        padding= []\n","        for k in range(0,len(ans_test)):\n","            temp = []\n","            temp = list(ans_test[k][i])\n","            padding.append(temp)\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        paddings_test.append(padded)\n","  \n","  #NUMERICAL COLUMNS\n","  numerical_columns = cls_encoder_args['dynamic_num_cols']+cls_encoder_args['static_num_cols']\n","  numerical_columns.remove('timesincelastevent')\n"," \n","  numerical_columns.append('Case ID')\n","  ans_test2 = groupby_caseID(dt_test_prefixes, numerical_columns )\n","  numerical_columns.remove('Case ID')\n","  pad_test  = []\n","  \n","  for i in numerical_columns:\n","        padding = []\n","        for k in range(0,len(ans_test2)):\n","            temp_test = []\n","            temp_test = list(ans_test2[k][i])\n","            padding.append(temp_test)\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        if dt_test_prefixes[i].max() !=0:\n","            padded = padded/dt_test_prefixes[i].max()\n","        else:\n","            padded = padded\n","        pad_test.append(padded)\n","  \n","  #TIME COLUMN\n","  ans_time_test = groupby_caseID(dt_test_prefixes,['timesincelastevent', 'Case ID'])\n","  cols = ['timesincelastevent'] \n","  pad_time_test = []\n","  for i in cols:\n","        padding = []\n","        for k in range(0,len(ans_time_test)):\n","            temp_test = []\n","            temp_test = list(ans_time_test[k][i])\n","            padding.append(temp_test)\n","\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        if dt_test_prefixes[i].max() !=0:\n","            padded = padded/dt_test_prefixes[i].max()\n","        else:\n","            padded = padded\n","        pad_time_test.append(padded)\n","  \n","  padded_time_test=  reshape_num_data(pad_time_test[0], cutoff)\n","            \n","  return pad_test, paddings_test, padded_time_test\n","\n","def distance_FC(lista, listb):\n","    runsum = 0.0\n","    for a, b in zip(lista, listb):\n","        # square the distance of each\n","        #  then add them back into the sum\n","        runsum += abs(b - a)   \n","\n","    # square root it\n","    return runsum \n","\n","def functional_complexity(test_data, n_instances):\n","    NF_event=0\n","    NF_case=0\n","    NF_control=0\n","    new_model_inputs_test = []\n","    \n","    \n","    #the original prediction, flattened\n","    pred1 = model.predict(model_inputs_test)\n","    \n","    lst3 = []\n","    lst3.extend(pred1)\n","    flat_pred1 = [round(item) for sublist in lst3 for item in sublist]\n","      \n","    ###EVENT COLUMNS###\n","    print('event columns')\n","    result2 = test_data.copy()\n","\n","    for j in event_columns:\n","        new_items = []\n","        permuted_values  = set(result2[j].values)\n","        for i in range(0,n_instances):\n","            value = result2[j].iloc[i]\n","            permuted_list = np.setdiff1d(list(permuted_values),[value])\n","            if len(permuted_list)<1:\n","                random_value = value\n","            else:\n","                random_value = random.choice(permuted_list)\n","            new_items.append(random_value)\n","        \n","        result2[j] = new_items\n"," \n","    new_pad_test, new_paddings_test, new_padded_time_test = create_test_data(result2)\n","        \n","     #add the permuted padded data to the new_model_inputs list\n","    for i in range(0,len(new_paddings_test)):\n","        new_model_inputs_test.append(new_paddings_test[i])\n","\n","    for i in range(0,len(pad_test)):\n","        new_model_inputs_test.append(reshape_num_data(new_pad_test[i], cutoff))\n","    \n","    #add padded time\n","    new_model_inputs_test.append(new_padded_time_test)\n","\n","   \n","    pred2 = model.predict(new_model_inputs_test)\n","    lst3 = []\n","    lst3.extend(pred2)\n","    flat_pred2 = [round(item) for sublist in lst3 for item in sublist]\n","  \n","    \n","    NF_event = distance_FC(flat_pred1, flat_pred2)\n","    print(NF_event, n_instances)\n","    FC_event = NF_event/(n_instances)\n","    print('FC_event: ', FC_event)\n","\n","    ###CASE COLUMNS###\n","    print('case columns')\n","    result2 = test_data.copy()\n","    new_model_inputs_test = []\n","    for j in case_columns:\n","        new_items = []\n","        permuted_values  = set(result2[j].values)\n","        for i in range(0,n_instances):\n","            value = result2[j].iloc[i]\n","            permuted_list = np.setdiff1d(list(permuted_values),[value])\n","            if len(permuted_list)<1:\n","                random_value = value\n","            else:\n","                random_value = random.choice(permuted_list)\n","            new_items.append(random_value)\n","        \n","        result2[j] = new_items\n","\n","    new_pad_test, new_paddings_test, new_padded_time_test = create_test_data(result2)\n","        \n","     #add the permuted padded data to the new_model_inputs list\n","    for i in range(0,len(new_paddings_test)):\n","        new_model_inputs_test.append(new_paddings_test[i])\n","\n","    for i in range(0,len(pad_test)):\n","        new_model_inputs_test.append(reshape_num_data(new_pad_test[i], cutoff))\n","    \n","    #add padded time\n","    new_model_inputs_test.append(new_padded_time_test)\n","\n","    pred2 = model.predict(new_model_inputs_test) \n","   \n","    lst3 = []\n","    lst3.extend(pred2)\n","    flat_pred3 = [round(item) for sublist in lst3 for item in sublist]\n","  \n","    \n","    NF_case = distance_FC(flat_pred1, flat_pred3)\n","    print(NF_case, n_instances)\n","    FC_case = NF_case/(n_instances)\n","    print('FC_case: ', FC_case)\n","    \n","    \n","    #CONTROLFLOW COLUMNS###\n","    print('control columns')\n","    result2 = test_data.copy()\n","    new_model_inputs_test = []\n","\n","    for j in controlflow_columns:\n","        new_items = []\n","        permuted_values  = set(result2[j].values)\n","        for i in range(0,n_instances):\n","            value = result2[j].iloc[i]\n","            permuted_list = np.setdiff1d(list(permuted_values),[value])\n","            if len(permuted_list)<1:\n","                random_value = value\n","            else:\n","                random_value = random.choice(permuted_list)\n","            new_items.append(random_value)\n","        \n","        result2[j] = new_items\n","\n","    new_pad_test, new_paddings_test, new_padded_time_test = create_test_data(result2)\n","        \n","     #add the permuted padded data to the new_model_inputs list\n","    for i in range(0,len(new_paddings_test)):\n","        new_model_inputs_test.append(new_paddings_test[i])\n","\n","    for i in range(0,len(pad_test)):\n","        new_model_inputs_test.append(reshape_num_data(new_pad_test[i], cutoff))\n","    \n","    #add padded time\n","    new_model_inputs_test.append(new_padded_time_test)\n","\n","    pred2 = model.predict(new_model_inputs_test)\n","  \n","    lst3 = []\n","    lst3.extend(pred2)\n","    flat_pred4 = [round(item) for sublist in lst3 for item in sublist]\n","  \n","    \n","    NF_control = distance_FC(flat_pred1, flat_pred4)\n","    print(NF_control, n_instances)\n","    FC_control = NF_control/(n_instances)\n","    print('FC_control: ', FC_control)\n","    \n","    return FC_event, FC_case, FC_control"]},{"cell_type":"markdown","metadata":{"id":"vTmgzrV9x5vk"},"source":["# **Monotonicity**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hk0Mf6jSx54Z"},"outputs":[],"source":["def numbers_list():\n","    numbers = []\n","    for i in cat_cols:\n","        no_cols = len(data.groupby([i]))+1\n","        numbers.append(no_cols)\n","\n","    for i in numerical_columns:\n","        numbers.append(1)\n","\n","    #time layer\n","    numbers.append(1)\n","    return numbers\n","\n","def data_sample(model_inputs, sample_length):\n","    sample_data = []\n","    for i in range(0, len(model_inputs)):\n","        sample_data.append(model_inputs[i][0:sample_length])\n","    return sample_data\n","\n","\n","def monotonicity(attention_values, effects_saved):\n","    len(attention_values)\n","    # prepare data\n","    coef, p = spearmanr(attention_values, effects_saved)\n","    print('lengths', len(attention_values), len(effects_saved))\n","    print('Spearmans correlation coefficient: %.3f' % coef)\n","    # interpret the significance\n","    alpha = 0.05\n","    if p > alpha:\n","        print('Samples are uncorrelated (fail to reject H0) p=%.3f' % p)\n","    else:\n","        print('Samples are correlated (reject H0) p=%.3f' % p)\n","\n","    #feature importance of original model\n","    feature_importance=pd.DataFrame()\n","    columns = cat_cols + numerical_columns\n","    columns.append('time')\n","    feature_importance['columns']=columns\n","    feature_importance['importances'] = effects_saved\n","    attention_importances=pd.DataFrame()\n","    attention_importances['columns']=columns\n","    attention_importances['importances_attention'] = attention_values\n","\n","    #resulting frame\n","    resulting_frame = pd.concat([attention_importances, feature_importance], join='inner', axis=1)\n","    resulting_frame.sort_values(by='importances',ascending=False,inplace=True)\n","\n","     #top 10 of attention values importances and extract which columns are in it\n","    attention_importances.sort_values(by='importances_attention',ascending=False,inplace=True)\n","    attention_top_10 = attention_importances[:10]\n","    #attention_event\n","    attention_event = len(attention_top_10[attention_top_10['columns'].isin(event_columns)])\n","    #attention_case\n","    attention_case = len(attention_top_10[attention_top_10['columns'].isin(case_columns)])\n","    #attention_control\n","    attention_control = len(attention_top_10[attention_top_10['columns'].isin(controlflow_columns)])\n","    \n","    #similar for original model\n","    feature_importance.sort_values(by='importances',ascending=False,inplace=True)\n","    feature_top_10 = feature_importance[:10]\n","    #feature-event\n","    feature_event = len(feature_top_10[feature_top_10['columns'].isin(event_columns)])\n","    #feature-case\n","    feature_case = len(feature_top_10[feature_top_10['columns'].isin(case_columns)])\n","    #feature_control\n","    feature_control = len(feature_top_10[feature_top_10['columns'].isin(controlflow_columns)])\n","\n","    #LOD\n","    model_importance = [feature_event, feature_case, feature_control]\n","    explainability_importance = [attention_event, attention_case, attention_control]\n","    LOD = distance.euclidean(model_importance, explainability_importance)\n","\n","    return coef, LOD"]},{"cell_type":"markdown","metadata":{"id":"Xx3FVtgkOgyq"},"source":["# **parameters**"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Cnlhy3XIOgof"},"outputs":[],"source":["# parameters\n","#terminology \n","results_dir = './results__dir_DL' \n","params_dir = './params_dir_DL'\n","logs_dir = '/data/leuven/365/vsc36567/xAI-PPM/data/labeled_logs_csv_processed'\n","column_selection= 'all'\n","cls_encoding ='embeddings'\n","classifiers =['LSTM']\n","n_iter = 1\n","n_splits = 3\n","train_ratio = 0.8\n","random_state = 22\n","l2reg = 0.001\n","\n","dataset_ref_to_datasets = {\n","    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(1,6)],\n","}\n","datasets = []\n","for k, v in dataset_ref_to_datasets.items():\n","    datasets.extend(v)\n","\n","allow_negative = False\n","incl_time = True \n","incl_res = True\n","# create results directory\n","if not os.path.exists(os.path.join(results_dir)):\n","    os.makedirs(os.path.join(results_dir))"]},{"cell_type":"markdown","metadata":{"id":"UpCBthWhMhkS"},"source":["# **Function for preprocessing the data**"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["#### BPIC2015 settings ####\n","\n","case_id_col = {}\n","activity_col = {}\n","resource_col = {}\n","timestamp_col = {}\n","label_col = {}\n","pos_label = {}\n","neg_label = {}\n","dynamic_cat_cols = {}\n","static_cat_cols = {}\n","dynamic_num_cols = {}\n","static_num_cols = {}\n","filename = {}\n","\n","\n","#### BPIC2015 settings ####\n","for municipality in range(1,6):\n","    for formula in range(1,3):\n","        dataset = \"bpic2015_%s_f%s\"%(municipality, formula)\n","        \n","        filename[dataset] = os.path.join(logs_dir, \"BPIC15_%s_f%s.csv\"%(municipality, formula))\n","\n","        case_id_col[dataset] = \"Case ID\"\n","        activity_col[dataset] = \"Activity\"\n","        resource_col[dataset] = \"org:resource\"\n","        timestamp_col[dataset] = \"time:timestamp\"\n","        label_col[dataset] = \"label\"\n","        pos_label[dataset] = \"deviant\"\n","        neg_label[dataset] = \"regular\"\n","\n","        # features for classifier\n","        dynamic_cat_cols[dataset] = [\"Activity\", \"monitoringResource\", \"question\", \"org:resource\"]\n","        static_cat_cols[dataset] = [\"Responsible_actor\"]\n","        dynamic_num_cols[dataset] = [\"hour\", \"weekday\", \"month\", \"timesincemidnight\", \"timesincelastevent\", \"timesincecasestart\", \"event_nr\", \"open_cases\"]\n","        static_num_cols[dataset] = [\"SUMleges\", 'Aanleg (Uitvoeren werk of werkzaamheid)', 'Bouw', 'Brandveilig gebruik (vergunning)', 'Gebiedsbescherming', 'Handelen in strijd met regels RO', 'Inrit/Uitweg', 'Kap', 'Milieu (neutraal wijziging)', 'Milieu (omgevingsvergunning beperkte milieutoets)', 'Milieu (vergunning)', 'Monument', 'Reclame', 'Sloop']\n","        \n","        if municipality in [3,5]:\n","            static_num_cols[dataset].append('Flora en Fauna')\n","        if municipality in [1,2,3,5]:\n","            static_num_cols[dataset].append('Brandveilig gebruik (melding)')\n","            static_num_cols[dataset].append('Milieu (melding)')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from easydict import EasyDict as edict\n","\n","ds_confs = edict({\n","    'case_id_col': case_id_col,\n","    'activity_col': activity_col,\n","    'resource_col': resource_col,\n","    'timestamp_col': timestamp_col,\n","    'label_col' : label_col,\n","    'pos_label' : pos_label,\n","    'neg_label' : neg_label,\n","    'dynamic_cat_cols': dynamic_cat_cols,\n","    'static_cat_cols': static_cat_cols,\n","    'dynamic_num_cols': dynamic_num_cols,\n","    'static_num_cols': static_num_cols,\n","    \n","    'filename':filename\n","})"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"JqpKGwadG0sH"},"outputs":[],"source":["def create_data(dt_train_prefixes, dt_test_prefixes):\n","  #get the label of the train and test set\n","  test_y = dataset_manager.get_label_numeric(dt_test_prefixes)\n","  train_y = dataset_manager.get_label_numeric(dt_train_prefixes)   \n","  \n","  #cat columns integerencoded\n","  cat_cols = cls_encoder_args['dynamic_cat_cols']+cls_encoder_args['static_cat_cols']\n","\n","  dt_train_prefixes[cat_cols],dt_test_prefixes[cat_cols]= prepare_inputs(dt_train_prefixes[cat_cols], dt_test_prefixes[cat_cols], data)\n","  dt_train_prefixes[cat_cols] = dt_train_prefixes[cat_cols]+1\n","  dt_test_prefixes[cat_cols] = dt_test_prefixes[cat_cols]+1\n","  #append caseId and label\n","  cat_cols.append('Case ID')\n","  cat_cols.append('label')\n","  #groupby case ID\n","  \n","  ans_train = groupby_caseID(dt_train_prefixes, cat_cols)\n","  ans_test = groupby_caseID(dt_test_prefixes, cat_cols)\n","  #obtain the new label lists after grouping\n","  train_y, test_y = labels_after_grouping(ans_train, ans_test)\n","  #remove then back\n","  cat_cols.remove('label')\n","  cat_cols.remove('Case ID')\n","  #pad cat columns\n","  paddings_train, paddings_test = pad_cat_data(cat_cols, ans_train, ans_test, maxlen)\n","  \n","  #NUMERICAL COLUMNS\n","  numerical_columns = cls_encoder_args['dynamic_num_cols']+cls_encoder_args['static_num_cols']\n","  numerical_columns.remove('timesincelastevent')\n"," \n","  numerical_columns.append('Case ID')\n","  ans_train2 = groupby_caseID(dt_train_prefixes, numerical_columns)\n","  ans_test2 = groupby_caseID(dt_test_prefixes, numerical_columns )\n","  numerical_columns.remove('Case ID')  \n","  pad_train, pad_test = pad_num_data(numerical_columns, ans_train2, ans_test2, maxlen, dt_train_prefixes, dt_test_prefixes)\n","  \n","  #time inputs                   \n","  ans_time_train= groupby_caseID(dt_train_prefixes,['timesincelastevent', 'Case ID'])\n","  ans_time_test = groupby_caseID(dt_test_prefixes,['timesincelastevent', 'Case ID'])\n","  pad_time_train, pad_time_test = pad_num_data(['timesincelastevent'], ans_time_train, ans_time_test, maxlen, dt_train_prefixes, dt_test_prefixes)\n","  #reshape the time input\n","  padded_time = reshape_num_data(pad_time_train[0], cutoff)\n","  padded_time_test=  reshape_num_data(pad_time_test[0], cutoff)\n","            \n","  return pad_train, pad_test, paddings_train, paddings_test, padded_time, padded_time_test, train_y, test_y"]},{"cell_type":"markdown","metadata":{"id":"PVX1WhP2Mvv-"},"source":["# **loop over datasets and classifiers**"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["args = {'dropout_rate': 0.1 , \n","        'lstm_size': 2,\n","        'optimizer': 'Adam',\n","        'learning_rate': 0.001,\n","        'batch_size': 64,\n","        'epochs': 10}"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1rX07Ni_KeZL08O6IC6tOZd0drsvC8TY8"},"executionInfo":{"elapsed":32300005,"status":"ok","timestamp":1669245093199,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"},"user_tz":-60},"id":"agyg5xxq09yo","outputId":"fb848c45-4502-4365-aae6-48a23a90eab1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset: bpic2015_1_f2\n","Classifier LSTM\n","Encoding embeddings\n"]},{"name":"stderr","output_type":"stream","text":["/data/leuven/365/vsc36567/miniconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n","I0000 00:00:1744725341.795175 1399954 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0\n","/data/leuven/365/vsc36567/miniconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n","/data/leuven/365/vsc36567/miniconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n","/data/leuven/365/vsc36567/miniconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n","/data/leuven/365/vsc36567/miniconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["/data/leuven/365/vsc36567/miniconda3/lib/python3.12/site-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n","Expected: [['Activity', 'monitoringResource', 'question', 'org_resource', 'Responsible_actor', 'hour', 'weekday', 'month', 'timesincemidnight', 'timesincecasestart', 'event_nr', 'open_cases', 'SUMleges', 'Aanleg__Uitvoeren_werk_of_werkzaamheid_', 'Bouw', 'Brandveilig_gebruik__vergunning_', 'Gebiedsbescherming', 'Handelen_in_strijd_met_regels_RO', 'Inrit_Uitweg', 'Kap', 'Milieu__neutraal_wijziging_', 'Milieu__omgevingsvergunning_beperkte_milieutoets_', 'Milieu__vergunning_', 'Monument', 'Reclame', 'Sloop', 'Brandveilig_gebruik__melding_', 'Milieu__melding_', 'time_input']]\n","Received: inputs=('Tensor(shape=(None, 40))', 'Tensor(shape=(None, 40))', 'Tensor(shape=(None, 40))', 'Tensor(shape=(None, 40))', 'Tensor(shape=(None, 40))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))', 'Tensor(shape=(None, 40, 1))')\n","  warnings.warn(msg)\n","I0000 00:00:1744725349.961532 1414778 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"]},{"name":"stdout","output_type":"stream","text":["258/258 - 14s - 56ms/step - auc: 0.8533 - loss: 0.3820 - val_auc: 0.8708 - val_loss: 0.2825 - learning_rate: 1.0000e-03\n","Epoch 2/10\n","258/258 - 6s - 23ms/step - auc: 0.9651 - loss: 0.1967 - val_auc: 0.8682 - val_loss: 0.3431 - learning_rate: 1.0000e-03\n","Epoch 3/10\n","258/258 - 5s - 20ms/step - auc: 0.9716 - loss: 0.1763 - val_auc: 0.8555 - val_loss: 0.3465 - learning_rate: 1.0000e-03\n","Epoch 4/10\n","258/258 - 5s - 19ms/step - auc: 0.9772 - loss: 0.1593 - val_auc: 0.8553 - val_loss: 0.3816 - learning_rate: 1.0000e-03\n","Epoch 5/10\n","258/258 - 5s - 19ms/step - auc: 0.9797 - loss: 0.1506 - val_auc: 0.8608 - val_loss: 0.3848 - learning_rate: 1.0000e-03\n","Epoch 6/10\n","258/258 - 5s - 19ms/step - auc: 0.9820 - loss: 0.1434 - val_auc: 0.8565 - val_loss: 0.3747 - learning_rate: 1.0000e-03\n","Epoch 7/10\n","258/258 - 5s - 20ms/step - auc: 0.9846 - loss: 0.1340 - val_auc: 0.8633 - val_loss: 0.3900 - learning_rate: 1.0000e-03\n","Epoch 8/10\n","258/258 - 5s - 19ms/step - auc: 0.9859 - loss: 0.1278 - val_auc: 0.8512 - val_loss: 0.4143 - learning_rate: 1.0000e-03\n","Epoch 9/10\n","258/258 - 5s - 20ms/step - auc: 0.9874 - loss: 0.1215 - val_auc: 0.8440 - val_loss: 0.4418 - learning_rate: 1.0000e-03\n","Epoch 10/10\n","258/258 - 5s - 19ms/step - auc: 0.9885 - loss: 0.1164 - val_auc: 0.8480 - val_loss: 0.4237 - learning_rate: 1.0000e-03\n"]},{"name":"stderr","output_type":"stream","text":["/data/leuven/365/vsc36567/miniconda3/lib/python3.12/site-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n","Expected: [['Activity', 'monitoringResource', 'question', 'org_resource', 'Responsible_actor', 'hour', 'weekday', 'month', 'timesincemidnight', 'timesincecasestart', 'event_nr', 'open_cases', 'SUMleges', 'Aanleg__Uitvoeren_werk_of_werkzaamheid_', 'Bouw', 'Brandveilig_gebruik__vergunning_', 'Gebiedsbescherming', 'Handelen_in_strijd_met_regels_RO', 'Inrit_Uitweg', 'Kap', 'Milieu__neutraal_wijziging_', 'Milieu__omgevingsvergunning_beperkte_milieutoets_', 'Milieu__vergunning_', 'Monument', 'Reclame', 'Sloop', 'Brandveilig_gebruik__melding_', 'Milieu__melding_', 'time_input']]\n","Received: inputs=('Tensor(shape=(32, 40))', 'Tensor(shape=(32, 40))', 'Tensor(shape=(32, 40))', 'Tensor(shape=(32, 40))', 'Tensor(shape=(32, 40))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))', 'Tensor(shape=(32, 40, 1))')\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step\n"]}],"source":["cls_method = 'LSTM'\n","for dataset_name in datasets[:1]:\n","        print('Dataset:', dataset_name)\n","        print('Classifier', cls_method)\n","        print('Encoding', cls_encoding)\n","        dataset_manager = DatasetManager(dataset_name, ds_confs)\n","        data = dataset_manager.read_dataset() \n","        method_name = \"%s_%s\"%(column_selection,cls_encoding)\n","\n","        cls_encoder_args = {'case_id_col': dataset_manager.case_id_col, \n","                    'static_cat_cols': dataset_manager.static_cat_cols,\n","                    'static_num_cols': dataset_manager.static_num_cols, \n","                    'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n","                    'dynamic_num_cols': dataset_manager.dynamic_num_cols, \n","                    'fillna': True}\n","        \n","        #file to save results\n","        outfile = os.path.join(results_dir, \"performance_results_%s_%s_%s.csv\" % (cls_method, dataset_name, method_name))\n","    \n","        # determine min and max (truncated) prefix lengths\n","        min_prefix_length = 1\n","        max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n","        maxlen = cutoff = max_prefix_length\n","        \n","        # split into training and test\n","        train, test = dataset_manager.split_data_strict(data, train_ratio, split=\"temporal\")\n","        \n","        #prefix generation of train and test data\n","        dt_train_prefixes = dataset_manager.generate_prefix_data(train, min_prefix_length, max_prefix_length)\n","        dt_test_prefixes = dataset_manager.generate_prefix_data(test, min_prefix_length, max_prefix_length)\n","\n","        pad_train, pad_test, paddings_train, paddings_test, padded_time, padded_time_test, train_y, test_y = create_data(dt_train_prefixes, dt_test_prefixes)\n","        cat_cols = cls_encoder_args['dynamic_cat_cols']+cls_encoder_args['static_cat_cols']\n","    \n","        numerical_columns = cls_encoder_args['dynamic_num_cols']+cls_encoder_args['static_num_cols']\n","        numerical_columns.remove('timesincelastevent')\n","\n","        #control flow, event and case \n","        event_columns = cls_encoder_args['dynamic_cat_cols']+cls_encoder_args['dynamic_num_cols']\n","        controlflow_columns =  [x for x in event_columns if 'Activity' in x]\n","        event_columns.remove(controlflow_columns[0])\n","        case_columns = cls_encoder_args['static_cat_cols']+cls_encoder_args['static_num_cols']\n","        len_event = len(event_columns)\n","        len_case = len(case_columns)\n","        len_control = len(controlflow_columns)\n","\n","        #create the input layers and embeddings\n","        embeddings= []\n","        input_layers = []\n","        preds_all = []\n","        nr_events_all = []\n","        test_y_all = []\n","        score = 0\n","        dim = 0\n","        test_y_all = []\n","        test_y_all.extend(test_y)\n","        nr_events = list(dataset_manager.get_prefix_lengths(dt_test_prefixes))\n","        nr_events_all.extend(nr_events)\n","        \n","        for i in cat_cols:\n","            cat_weights, index_cat, cat_index, no_cols = create_indexes(i, data)\n","            i = i.replace(':','_')\n","            i = i.replace(' ','_')\n","            input_layer = Input(shape=(cutoff,), name=i)\n","            embedding = Embedding(cat_weights.shape[0],\n","                                  cat_weights.shape[1],\n","                                  weights=[cat_weights],\n","                                  input_shape=no_cols,\n","                                  name='embed_'+i)(input_layer)\n","            embeddings.append(embedding)\n","            input_layers.append(input_layer)\n","            dim += cat_weights.shape[1]\n","\n","        #static input layers\n","\n","        for j in numerical_columns:\n","            j = j.replace('(','_')\n","            j = j.replace(')','_')\n","            j = j.replace(' ','_')\n","            j = j.replace(':','_')\n","            j = j.replace('/','_')\n","            input_layer = Input(shape=(cutoff,1), name=j)\n","            input_layers.append(input_layer)\n","            embeddings.append(input_layer)\n","            dim +=1\n","\n","        #create the model inputs\n","        model_inputs= []\n","        model_inputs_test= []\n","        for i in range(0,len(paddings_train)):\n","                model_inputs.append(paddings_train[i])\n","\n","        for i in range(0,len(paddings_test)):\n","                model_inputs_test.append(paddings_test[i])\n","\n","        for i in range(0,len(pad_train)):\n","                model_inputs.append(reshape_num_data(pad_train[i], cutoff))\n","\n","        for i in range(0,len(pad_test)):\n","                model_inputs_test.append(reshape_num_data(pad_test[i], cutoff))\n","\n","        model_inputs.append(padded_time)\n","        model_inputs_test.append(padded_time_test)\n","\n","            #Apply dropout on inputs\n","        full_embs = concatenate(embeddings, name='full_embedding')\n","        full_embs = Dropout(args['dropout_rate'])(full_embs)\n","        time_input_layer = Input(shape=(cutoff,1), name='time_input')\n","        input_layers.append(time_input_layer)\n","        time_embs = concatenate([full_embs, time_input_layer], name='allInp')\n","        dim += 1\n","        l2reg=0.001\n","\n","            \n","        # Building a LSTM: Compute alpha, visit attention\n","        alpha = Bidirectional(LSTM(args['lstm_size'], return_sequences=True), name='alpha')\n","        alpha_out = alpha(time_embs)\n","        alpha_dense = Dense(1, kernel_regularizer=l2(l2reg))\n","        alpha_out = TimeDistributed(alpha_dense, name='alpha_dense_0')(alpha_out)\n","        alpha_out = Softmax(axis=1, name='alpha_softmax')(alpha_out)\n","        \n","        \n","        #Compute beta, codes attention\n","        beta = Bidirectional(LSTM(args['lstm_size'], return_sequences=True),   name='beta')\n","        beta_out = beta(time_embs)\n","        beta_dense = Dense(dim, activation='tanh', kernel_regularizer=l2(l2reg))\n","        beta_out = TimeDistributed(beta_dense, name='beta_dense_0')(beta_out)\n","        \n","        #Compute context vector based on attentions and embeddings\n","        c_t = Multiply()([alpha_out, beta_out, time_embs])\n","        c_t = Lambda(lambda x: backend.sum(x, axis=1))(c_t)\n","\n","        #Make a prediction\n","        contexts = Dropout(args['dropout_rate'])(c_t)\n","        output_layer = Dense(1, activation='sigmoid', name='final_output')(contexts)\n","            \n","            #MODEL\n","        model = Model(inputs=[input_layers], outputs=output_layer)\n","\n","        if args['optimizer']=='RMSprop':\n","                opt = RMSprop(learning_rate=args['learning_rate'])\n","        if args['optimizer']=='Nadam':\n","                opt = Nadam(learning_rate=args['learning_rate'])\n","        if args['optimizer']=='Adam':\n","                opt = Adam(learning_rate=args['learning_rate'])\n","        if args['optimizer']=='SGD':\n","                opt = SGD(learning_rate=args['learning_rate'])\n","\n","        model.compile(loss={'final_output':'binary_crossentropy'}, \n","                      optimizer= opt,\n","                      metrics=[tf.keras.metrics.AUC(name='auc')])\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=42)\n","        model_checkpoint = ModelCheckpoint('output/lstm_{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', \n","                                            verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n","        lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, \n","                                       verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n","\n","        result = model.fit(model_inputs,\n","                np.array(train_y),\n","                callbacks=[early_stopping, lr_reducer],\n","                validation_split = 0.1,\n","                verbose=2, \n","                batch_size=args['batch_size'],\n","                epochs=args['epochs'])\n","        pred = model.predict(model_inputs_test)\n","        preds_all.extend(pred)\n","        auc_total = roc_auc_score(test_y_all, preds_all)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1669212693667,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"},"user_tz":-60},"id":"4y86PNWo_a5B","outputId":"fd438c90-338d-4c71-e654-3888d6159841"},"outputs":[{"name":"stdout","output_type":"stream","text":["hello\n"]}],"source":["print('hello')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vOo9wnWbwc8"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["4SGhW_-Gygr-","UMxd6y3fIACe","v0yRQ_ZSMaFG","yDqFdSCNzg87","iSXBB6FWOyRN","i00WpG-jxewE","t6qDWGGTxptx","3m4jrc3axxDj","vTmgzrV9x5vk"],"machine_shape":"hm","provenance":[{"file_id":"1xVEDFKM5kGpsOGGOiF2LcT2I6uTlp38J","timestamp":1640767980582},{"file_id":"1ifDBBsMYqN51m5Lm-JvSWmcuoVu9AGU3","timestamp":1638807074915}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":0}
