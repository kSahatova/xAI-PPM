{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_column_schema import DatasetSchemas \n",
    "from dataset_manager_optimized import DatasetManager, CVFoldsManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from experiment_runner import MLExperimentRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir =  r'C:\\Users\\sahat\\OneDrive - KU Leuven\\Research\\PPM&xAI\\data'\n",
    "# '/data/leuven/365/vsc36567/xAI-PPM/data/processed_benchmark_event_logs'\n",
    "\n",
    "ds_name = 'bpic2017' \n",
    "ds_file_names = ['BPIC17_O_Accepted.csv']#, 'BPIC17_O_Cancelled.csv', 'BPIC17_O_Refused.csv']\n",
    "bpic17_column_schema = DatasetSchemas.bpic2017()\n",
    "\n",
    "bucketing_method = 'single'\n",
    "encoding_method = 'agg'\n",
    "encoding_dict = {\n",
    "            \"laststate\": [\"static\", \"last\"],\n",
    "            \"agg\": [\"static\", \"agg\"],\n",
    "            \"index\": [\"static\", \"index\"],\n",
    "            \"combined\": [\"static\", \"last\", \"agg\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_args = bpic17_column_schema.get_encoder_args(fillna=True)\n",
    "\n",
    "cls_method = 'rf'\n",
    "cls_args = {'n_estimators': 500, \n",
    "            'max_features': 'sqrt',\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dataset manager\n",
    "\n",
    "dm = DatasetManager(ds_name, ds_column_schema=bpic17_column_schema) \n",
    "data = dm.read_dataset(osp.join(data_dir, ds_file_names[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_runner = MLExperimentRunner(ds_name, dm, bucketing_method, encoding_dict[encoding_method],\n",
    "                                encoding_args, cls_method, cls_args, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train set:  (927785, 26) \n",
      "Shape of the test set:  (239791, 26)\n",
      "\n",
      "Generating train and test prefixes with the max length 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prefixes: 100%|██████████| 19/19 [00:07<00:00,  2.71it/s]\n",
      "Generating prefixes: 100%|██████████| 19/19 [00:01<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train prefixes:  5119334\n",
      "Length of the test prefixes:  1297206\n",
      "\n",
      "Creating buckets with the \"single\" bucket method\n",
      "    Processing bucket: 1\n",
      "       Shape of the train bucket and its labels after labels extraction:  (469762, 29) (469762,)\n",
      "       Shape of the test bucket and its labels after labels extraction:  (118532, 29) (118532,)\n",
      "\n",
      "       Shape of the train bucket after encoding:  (469762, 183)\n",
      "       Shape of the test bucket after encoding:  (118532, 183)\n",
      "    Finished processing bucket: 1\n"
     ]
    }
   ],
   "source": [
    "encoded_bucketed_data = exp_runner.preprocess_event_log(data, max_prefix_length=20, gap=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Fitting the created RandomForestClassifier classifier***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks      | elapsed:  9.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Estimating the fitted classifier***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks      | elapsed:    5.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC of the classifier on the bucket_1:  0.5010057813556278\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'bucket_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mexp_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_bucketed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mencoded_bucketed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moffline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PycharmProjects\\xAI-PPM\\experiment_runner.py:278\u001b[0m, in \u001b[0;36mMLExperimentRunner.run_experiment\u001b[1;34m(self, encoded_train, encoded_test, phase)\u001b[0m\n\u001b[0;32m    276\u001b[0m     bucket_auc \u001b[38;5;241m=\u001b[39m roc_auc_score(test_y, preds)\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROC-AUC of the classifier on the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_bucket_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m, bucket_auc)\n\u001b[1;32m--> 278\u001b[0m     \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_bucket_id\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m bucket_auc\n\u001b[0;32m    279\u001b[0m     result[test_bucket_id][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m classifier\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyError\u001b[0m: 'bucket_1'"
     ]
    }
   ],
   "source": [
    "result = exp_runner.run_experiment(encoded_bucketed_data['train'], \n",
    "                                   encoded_bucketed_data['test'], \n",
    "                                   phase='offline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# for file_name in ds_file_names:\n",
    "\n",
    "#     dm = DatasetManager(dataset_name, preprocessing_config)\n",
    "#     df = dm.read_dataset(osp.join(data_dir, file_name))\n",
    "#     min_prefix_length = preprocessing_config.min_prefix_length\n",
    "#     max_prefix_length = min(preprocessing_config.max_prefix_length, \n",
    "#                             dm.get_pos_case_length_quantile(df, 0.90))\n",
    "\n",
    "#     # Splitting the data into train and test set \n",
    "#     train, test = dm.split_data_strict(df, \n",
    "#                                         train_ratio=preprocessing_config.train_ratio, \n",
    "#                                         split='temporal')\n",
    "#     print('Shape of the train set: ', train.shape, '\\nShape of the test set: ', test.shape)\n",
    "    \n",
    "#     # Generating prefixes \n",
    "#     df_train_prefixes = dm.generate_prefix_data(test, min_prefix_length, max_prefix_length)\n",
    "#     df_test_prefixes = dm.generate_prefix_data(train, min_prefix_length, max_prefix_length)\n",
    "    \n",
    "#     # Create buckets for each prexif or a single one that fits all the prexifes  \n",
    "#     bucketer = get_bucketer(method=bucketing, case_id_col=case_id_col)\n",
    "#     train_bucket = bucketer.fit_predict(df_train_prefixes)\n",
    "#     test_bucket = bucketer.predict(df_test_prefixes)\n",
    "\n",
    "#     # Iterating over the set of generated buckets\n",
    "#     for bucket in set(test_bucket):\n",
    "\n",
    "#         train_bucket_ind = dm.get_indexes(df_train_prefixes)[bucket == train_bucket]\n",
    "#         test_bucket_ind = dm.get_indexes(df_test_prefixes)[bucket == test_bucket]   \n",
    "\n",
    "#         # extracting training data for the experiment\n",
    "#         df_train_bucket = dm.get_data_by_indexes(df_train_prefixes, train_bucket_ind)\n",
    "#         df_test_bucket = dm.get_data_by_indexes(df_test_prefixes, test_bucket_ind)\n",
    "        \n",
    "#         _, train_y = np.asarray(dm.get_labels(df_train_bucket))\n",
    "#         _, test_y = np.asarray(dm.get_labels(df_test_bucket))\n",
    "\n",
    "#         # Get a set of encoders for preprocessing of static and dynamic features\n",
    "#         featureCombinerExperiment = FeatureUnion(\n",
    "#                 [(enc_method, get_encoder(enc_method, **encoder_config)) for enc_method in encoding_dict[encoding]])\n",
    "        \n",
    "#         encoded_train_bucket = featureCombinerExperiment.fit_transform(df_train_bucket)\n",
    "#         encoded_test_bucket = featureCombinerExperiment.fit_transform(df_test_bucket)        \n",
    "\n",
    "\n",
    "#         enc_fnames = []\n",
    "#         for _, transformer in featureCombinerExperiment.transformer_list:\n",
    "#             for new_fname in transformer.get_feature_names():\n",
    "#                 enc_fnames.append(new_fname)\n",
    "#         # enc_fnames.append('encoded_label')\n",
    "\n",
    "#         # create a dataframe with the encoded training features and label\n",
    "#         # encoded_training = np.concatenate((encoded_training, train_y.reshape(-1, 1)), axis=1)\n",
    "#         training_set_df = pd.DataFrame(encoded_training, columns=enc_fnames)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
