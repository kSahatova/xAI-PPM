{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_column_schema import DatasetSchemas \n",
    "from dataset_manager_optimized import DatasetManager, CVFoldsManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "from model_trainer import ModelTrainingPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/data/leuven/365/vsc36567/xAI-PPM/data/processed_benchmark_event_logs'\n",
    "ds_name = 'bpic2017' \n",
    "ds_file_names = ['BPIC17_O_Accepted.csv']#, 'BPIC17_O_Cancelled.csv', 'BPIC17_O_Refused.csv']\n",
    "bpic17_column_schema = DatasetSchemas.bpic2017()\n",
    "\n",
    "bucketing_method = 'single'\n",
    "encoding_method = 'agg'\n",
    "encoding_dict = {\n",
    "            \"laststate\": [\"static\", \"last\"],\n",
    "            \"agg\": [\"static\", \"agg\"],\n",
    "            \"index\": [\"static\", \"index\"],\n",
    "            \"combined\": [\"static\", \"last\", \"agg\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_config = edict({'column_schema': bpic17_column_schema,\n",
    "                            'bucketing_method': 'single',\n",
    "                            'encoding_methods': encoding_dict[encoding_method],\n",
    "                            'encoding_args': bpic17_column_schema.get_encoder_args(fillna=True),\n",
    "                            'min_prefix_length': 1,\n",
    "                            'max_prefix_length': 20,\n",
    "                            'gap': 1,\n",
    "                            'train_ratio': 0.8})\n",
    "\n",
    "cls_method = 'rf'\n",
    "cls_args = {'n_estimators': 500, \n",
    "            'max_features': 10,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for file_name in ds_file_names:\n",
    "\n",
    "    dm = DatasetManager(dataset_name, preprocessing_config)\n",
    "    df = dm.read_dataset(osp.join(data_dir, file_name))\n",
    "    min_prefix_length = preprocessing_config.min_prefix_length\n",
    "    max_prefix_length = min(preprocessing_config.max_prefix_length, \n",
    "                            dm.get_pos_case_length_quantile(df, 0.90))\n",
    "\n",
    "    # Splitting the data into train and test set \n",
    "    train, test = dm.split_data_strict(df, \n",
    "                                        train_ratio=preprocessing_config.train_ratio, \n",
    "                                        split='temporal')\n",
    "    print('Shape of the train set: ', train.shape, '\\nShape of the test set: ', test.shape)\n",
    "    \n",
    "    # Generating prefixes \n",
    "    df_train_prefixes = dm.generate_prefix_data(test, min_prefix_length, max_prefix_length)\n",
    "    df_test_prefixes = dm.generate_prefix_data(train, min_prefix_length, max_prefix_length)\n",
    "    \n",
    "    # Create buckets for each prexif or a single one that fits all the prexifes  \n",
    "    bucketer = get_bucketer(method=bucketing, case_id_col=case_id_col)\n",
    "    train_bucket = bucketer.fit_predict(df_train_prefixes)\n",
    "    test_bucket = bucketer.predict(df_test_prefixes)\n",
    "\n",
    "    # Iterating over the set of generated buckets\n",
    "    for bucket in set(test_bucket):\n",
    "\n",
    "        train_bucket_ind = dm.get_indexes(df_train_prefixes)[bucket == train_bucket]\n",
    "        test_bucket_ind = dm.get_indexes(df_test_prefixes)[bucket == test_bucket]   \n",
    "\n",
    "        # extracting training data for the experiment\n",
    "        df_train_bucket = dm.get_data_by_indexes(df_train_prefixes, train_bucket_ind)\n",
    "        df_test_bucket = dm.get_data_by_indexes(df_test_prefixes, test_bucket_ind)\n",
    "        \n",
    "        _, train_y = np.asarray(dm.get_labels(df_train_bucket))\n",
    "        _, test_y = np.asarray(dm.get_labels(df_test_bucket))\n",
    "\n",
    "        # Get a set of encoders for preprocessing of static and dynamic features\n",
    "        featureCombinerExperiment = FeatureUnion(\n",
    "                [(enc_method, get_encoder(enc_method, **encoder_config)) for enc_method in encoding_dict[encoding]])\n",
    "        \n",
    "        encoded_train_bucket = featureCombinerExperiment.fit_transform(df_train_bucket)\n",
    "        encoded_test_bucket = featureCombinerExperiment.fit_transform(df_test_bucket)        \n",
    "\n",
    "\n",
    "        enc_fnames = []\n",
    "        for _, transformer in featureCombinerExperiment.transformer_list:\n",
    "            for new_fname in transformer.get_feature_names():\n",
    "                enc_fnames.append(new_fname)\n",
    "        # enc_fnames.append('encoded_label')\n",
    "\n",
    "        # create a dataframe with the encoded training features and label\n",
    "        # encoded_training = np.concatenate((encoded_training, train_y.reshape(-1, 1)), axis=1)\n",
    "        training_set_df = pd.DataFrame(encoded_training, columns=enc_fnames)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
