{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from easydict import EasyDict as edict\n",
    "from dataset_manager_optimized import DatasetManager\n",
    "from preprocessing.bucketing import get_bucketer\n",
    "from preprocessing.encoding import get_encoder\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_id_col = \"Case ID\"\n",
    "activity_col = \"Activity\"\n",
    "resource_col = 'org:resource'\n",
    "timestamp_col = 'time:timestamp'\n",
    "label_col = \"label\"\n",
    "pos_label = \"deviant\"\n",
    "neg_label = \"regular\"\n",
    "\n",
    "# These will be the targets of the classification task\n",
    "relevant_offer_events = [\"O_Cancelled\", \"O_Accepted\", \"O_Refused\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_freq_threshold = 10\n",
    "max_category_levels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for classifier\n",
    "# Categorical features \n",
    "dynamic_cat_cols = [activity_col, resource_col, 'Action', 'CreditScore', 'EventOrigin', 'lifecycle:transition',\n",
    "                   \"Accepted\", \"Selected\"] # i.e. event attributes\n",
    "static_cat_cols = ['ApplicationType', 'LoanGoal'] # i.e. case attributes that are known from the start\n",
    "\n",
    "# Numeric features\n",
    "dynamic_num_cols = ['FirstWithdrawalAmount', 'MonthlyCost', 'NumberOfTerms', 'OfferedAmount',\n",
    "                   \"timesincelastevent\", \"timesincecasestart\", \"timesincemidnight\", \"event_nr\", \"month\", \"weekday\", \"hour\",\n",
    "                    \"open_cases\"]\n",
    "static_num_cols = ['RequestedAmount']\n",
    "\n",
    "static_cols = static_cat_cols + static_num_cols + [case_id_col, label_col]\n",
    "dynamic_cols = dynamic_cat_cols + dynamic_num_cols + [timestamp_col]\n",
    "cat_cols = dynamic_cat_cols + static_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = edict({'columns': {'case_id_col': case_id_col,\n",
    "                           'activity_col': activity_col,\n",
    "                           'resource_col': resource_col,\n",
    "                           'timestamp_col': timestamp_col,\n",
    "                           'label_col': label_col,\n",
    "                           'pos_label_col': pos_label,\n",
    "                           'dynamic_cat_cols': dynamic_cat_cols,\n",
    "                           'static_cat_cols': static_cat_cols,\n",
    "                           'dynamic_num_cols': dynamic_num_cols,\n",
    "                           'static_num_cols': static_num_cols}\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the path to your 'data' folder \n",
    "data_dir = r'C:\\Users\\sahat\\OneDrive - KU Leuven\\Research\\PPM&xAI\\data\\processed_benchmark_event_logs'\n",
    "# r'/data/leuven/365/vsc36567/xAI-PPM/data' \n",
    "# dataset  #\"BPIC2017_O_Cancelled\", \"BPIC2017_O_Refused\"] were commented to fit the encoded data into available memory \n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2017\" : [\"BPIC17_O_Accepted\"]#, \"BPIC2017_O_Cancelled\", \"BPIC2017_O_Refused\"],\n",
    "}\n",
    "\n",
    "bucketing = 'single'\n",
    "encoding = 'agg'\n",
    "method_name = ('_').join([bucketing, encoding])\n",
    "\n",
    "encoding_dict = {\n",
    "    'agg' : ['static', 'agg'],\n",
    "    'index' : ['static', 'index']\n",
    "    }\n",
    "\n",
    "gap = 1\n",
    "train_ratio = 0.8\n",
    "min_cases_for_training = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A folder for encoded datasets should be created in your 'data' folder\n",
    "\n",
    "# if not os.path.exists(encoded_datasets_dir):\n",
    "#   os.makedirs(os.path.join(encoded_datasets_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train set:  (927785, 26) \n",
      "Shape of the test set:  (239791, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prefixes: 100%|██████████| 20/20 [00:01<00:00, 12.31it/s]\n",
      "Generating prefixes: 100%|██████████| 20/20 [00:06<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 25.0715 seconds\n",
      "Memory required to allocate train and test prefixes:\n",
      " train prefixes=4.8297 Gb, test prefixes=1.2242 Gb\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import sys\n",
    "\n",
    "# Read the datasets\n",
    "# DataManager splits the data and extracts prefixes \n",
    "\n",
    "dataset_name = 'bpic2017'\n",
    "train_prefixes = {}\n",
    "test_prefixes = {}\n",
    "\n",
    "start_time = time.time()\n",
    "for file_name in dataset_ref_to_datasets[dataset_name]:\n",
    "    activity = file_name.split('_')[-1]\n",
    "    file_path = os.path.join(data_dir, file_name + '.csv')\n",
    "    dm = DatasetManager(dataset_name, config)\n",
    "    df = dm.read_dataset(file_path)\n",
    "\n",
    "    min_prefix_length_final = 1\n",
    "    max_prefix_length_final = min(20, dm.get_pos_case_length_quantile(df, 0.90))\n",
    "\n",
    "    train, test = dm.split_data_strict(df, train_ratio=train_ratio, split='temporal')\n",
    "    print('Shape of the train set: ', train.shape, '\\nShape of the test set: ', test.shape)\n",
    "    \n",
    "    df_test_prefixes = dm.generate_prefix_data(test, min_prefix_length_final, max_prefix_length_final)\n",
    "    df_train_prefixes = dm.generate_prefix_data(train, min_prefix_length_final, max_prefix_length_final)\n",
    "\n",
    "    train_prefixes[activity] = df_train_prefixes\n",
    "    test_prefixes[activity] = df_test_prefixes\n",
    "\n",
    "    bucketer = get_bucketer(method=bucketing, case_id_col=case_id_col)\n",
    "    train_bucket = bucketer.fit_predict(df_train_prefixes)\n",
    "    test_bucket = bucketer.predict(df_test_prefixes)\n",
    "\n",
    "train_mem = sys.getsizeof(train_prefixes[activity]) / 1024 ** 3\n",
    "test_mem = sys.getsizeof(test_prefixes[activity]) / 1024 ** 3\n",
    "print('Completed in {:.4f} seconds'.format(time.time() - start_time))\n",
    "print('Memory required to allocate train and test prefixes:\\n train prefixes={:.4f} Gb, test prefixes={:.4f} Gb'.format(train_mem, test_mem)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train set encoding, commented for the test set for the sake of allocated memory\n",
    "\n",
    "nr_events_all = []\n",
    "current_online_times = []\n",
    "\n",
    "\n",
    "encoder_config = {'case_id_col': case_id_col,\n",
    "                'static_cat_cols': static_cat_cols,\n",
    "                'static_num_cols': static_num_cols,\n",
    "                'dynamic_cat_cols': dynamic_cat_cols,\n",
    "                'dynamic_num_cols': dynamic_num_cols,\n",
    "                'fillna': True}\n",
    "\n",
    "for bucket in set(test_bucket):\n",
    "\n",
    "    relevant_train_bucket = dm.get_indexes(df_train_prefixes)[bucket == train_bucket]\n",
    "    # relevant_test_bucket = dm.get_indexes(df_test_prefixes)[bucket == test_bucket]   \n",
    "\n",
    "    # extracting training data for the experiment\n",
    "    df_train_bucket = dm.get_data_by_indexes(df_train_prefixes, relevant_train_bucket)\n",
    "    # df_test_bucket = dm.get_data_by_indexes(df_test_prefixes, relevant_test_bucket)\n",
    "    \n",
    "    _, train_y = np.asarray(dm.get_labels(df_train_bucket))\n",
    "    #_, test_y = np.asarray(dm.get_labels(df_test_bucket))\n",
    "\n",
    "    # Get a set of encoders for preprocessing of static and dynamic features\n",
    "    featureCombinerExperiment = FeatureUnion(\n",
    "            [(enc_method, get_encoder(enc_method, **encoder_config)) for enc_method in encoding_dict[encoding]])\n",
    "    \n",
    "    encoded_training = featureCombinerExperiment.fit_transform(df_train_bucket)\n",
    "    # ffeatures = featureCombinerExperiment.get_feature_names_out()\n",
    "    ffeatures = []\n",
    "    for name, transformer in featureCombinerExperiment.transformer_list:\n",
    "        ffeatures.append(transformer.get_feature_names())\n",
    "    feat_num = len(ffeatures)\n",
    "    ffeatures.append('encoded_label')\n",
    "\n",
    "    # create a dataframe with the encoded training features and label\n",
    "    encoded_training = np.concatenate((encoded_training, train_y.T), axis=1)\n",
    "    training_set_df = pd.DataFrame(encoded_training, columns=ffeatures)\n",
    "    bkt_size = training_set_df.shape[0]\n",
    "\n",
    "    # # create a dataframe with the encoded test features and label\n",
    "    # encoded_testing_bucket = featureCombinerExperiment.fit_transform(df_test_bucket)\n",
    "    # encoded_testing_bucket = np.concatenate((encoded_testing_bucket,test_y.T), axis=1)\n",
    "    # testing_set_df = pd.DataFrame(encoded_testing_bucket, columns=ffeatures)\n",
    "    # test_bkt_size = testing_set_df.shape[0]\n",
    "    \n",
    "    # save the preprocessed data and corresponding metadata into a file \n",
    "    # outfile_train = 'encoded_training_%s_%s_%s_%s_%s.csv' % (\n",
    "    #                 dataset_name, method_name, bkt_size, prfx_len, feat_num)\n",
    "    # training_set_df.to_csv(os.path.join(encoded_datasets_dir, outfile_train), sep=';', columns=ffeatures, index=False)\n",
    "    # print('%s;%s;%s;%s;%s;%s\\n' % (dataset_name, method_name, 'training', bkt_size, prfx_len, feat_num))\n",
    "    \n",
    "    # outfile_test = 'encoded_testing_%s_%s_%s_%s_%s.csv' % (\n",
    "    #                 dataset_name, method_name, test_bkt_size, test_prfx_len, feat_num)\n",
    "    # testing_set_df.to_csv(os.path.join(encoded_datasets_dir, outfile_test), sep=';', columns=ffeatures, index=False)\n",
    "    # print('%s;%s;%s;%s;%s;%s\\n' % (dataset_name, method_name, 'testing', test_bkt_size, test_prfx_len, feat_num))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
