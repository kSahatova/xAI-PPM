{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from easydict import EasyDict as edict\n",
    "from dataset_manager import DatasetManager\n",
    "from preprocessing.bucketing import get_bucketer\n",
    "from preprocessing.encoding import get_encoder\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_id_col = \"Case ID\"\n",
    "activity_col = \"Activity\"\n",
    "resource_col = 'org:resource'\n",
    "timestamp_col = 'time:timestamp'\n",
    "label_col = \"label\"\n",
    "pos_label = \"deviant\"\n",
    "neg_label = \"regular\"\n",
    "\n",
    "# These will be the targets of the classification task\n",
    "relevant_offer_events = [\"O_Cancelled\", \"O_Accepted\", \"O_Refused\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_freq_threshold = 10\n",
    "max_category_levels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for classifier\n",
    "# Categorical features \n",
    "dynamic_cat_cols = [activity_col, resource_col, 'Action', 'CreditScore', 'EventOrigin', 'lifecycle:transition',\n",
    "                   \"Accepted\", \"Selected\"] # i.e. event attributes\n",
    "static_cat_cols = ['ApplicationType', 'LoanGoal'] # i.e. case attributes that are known from the start\n",
    "\n",
    "# Numeric features\n",
    "dynamic_num_cols = ['FirstWithdrawalAmount', 'MonthlyCost', 'NumberOfTerms', 'OfferedAmount',\n",
    "                   \"timesincelastevent\", \"timesincecasestart\", \"timesincemidnight\", \"event_nr\", \"month\", \"weekday\", \"hour\",\n",
    "                    \"open_cases\"]\n",
    "static_num_cols = ['RequestedAmount']\n",
    "\n",
    "static_cols = static_cat_cols + static_num_cols + [case_id_col, label_col]\n",
    "dynamic_cols = dynamic_cat_cols + dynamic_num_cols + [timestamp_col]\n",
    "cat_cols = dynamic_cat_cols + static_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = edict({'columns': {'case_id_col': case_id_col,\n",
    "                           'activity_col': activity_col,\n",
    "                           'resource_col': resource_col,\n",
    "                           'timestamp_col': timestamp_col,\n",
    "                           'label_col': label_col,\n",
    "                           'pos_label_col': pos_label,\n",
    "                           'dynamic_cat_cols': dynamic_cat_cols,\n",
    "                           'static_cat_cols': static_cat_cols,\n",
    "                           'dynamic_num_cols': dynamic_num_cols,\n",
    "                           'static_num_cols': static_num_cols}\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the path to your 'data' folder \n",
    "data_dir = r'/data/leuven/365/vsc36567/xAI-PPM/data' \n",
    "# dataset  #\"BPIC2017_O_Cancelled\", \"BPIC2017_O_Refused\"] were commented to fit the encoded data into available memory \n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2017\" : [\"BPIC17_O_Accepted\",] #\"BPIC2017_O_Cancelled\", \"BPIC2017_O_Refused\"],\n",
    "}\n",
    "\n",
    "bucketing = 'single'\n",
    "encoding = 'agg'\n",
    "method_name = ('_').join([bucketing, encoding])\n",
    "\n",
    "encoding_dict = {\n",
    "    'agg' : ['static', 'agg'],\n",
    "    'index' : ['static', 'index']\n",
    "    }\n",
    "\n",
    "gap = 1\n",
    "train_ratio = 0.5\n",
    "min_cases_for_training = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A folder for encoded datasets should be created in your 'data' folder\n",
    "encoded_datasets_dir = f'/data/leuven/365/vsc36567/xAI-PPM/data/encoded_datasets_{method_name}'\n",
    "if not os.path.exists(encoded_datasets_dir):\n",
    "  os.makedirs(os.path.join(encoded_datasets_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:06<00:00,  2.81it/s]\n",
      "100%|██████████| 19/19 [00:06<00:00,  2.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read the datasets\n",
    "# DataManager splits the data and extracts prefixes \n",
    "\n",
    "dataset_name = 'bpic2017'\n",
    "train_prefixes = {}\n",
    "test_prefixes = {}\n",
    "\n",
    "for file_name in dataset_ref_to_datasets[dataset_name]:\n",
    "    activity = file_name.split('_')[-1]\n",
    "    file_path = os.path.join(data_dir, file_name + '.csv')\n",
    "    dm = DatasetManager(dataset_name, config)\n",
    "    df = dm.read_dataset(file_path)\n",
    "\n",
    "    min_prefix_length_final = 1\n",
    "    max_prefix_length_final = min(20, dm.get_pos_case_length_quantile(df, 0.90))\n",
    "\n",
    "    train, test = dm.split_data_strict(df, train_ratio=train_ratio, split='temporal')\n",
    "    \n",
    "    df_test_prefixes = dm.generate_prefix_data(test, min_prefix_length_final, max_prefix_length_final)\n",
    "    df_train_prefixes = dm.generate_prefix_data(train, min_prefix_length_final, max_prefix_length_final)\n",
    "\n",
    "    train_prefixes[activity] = df_train_prefixes\n",
    "    test_prefixes[activity] = df_test_prefixes\n",
    "\n",
    "    bucketer = get_bucketer(method=bucketing, case_id_col=case_id_col)\n",
    "    train_bucket = bucketer.fit_predict(df_train_prefixes)\n",
    "    test_bucket = bucketer.predict(df_test_prefixes)\n",
    "    nr_events_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1013315/3848727303.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  test_prfx_len = dm.get_prefix_lengths(df_test_bucket)[0]\n",
      "/tmp/ipykernel_1013315/3848727303.py:29: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  prfx_len = dm.get_prefix_lengths(df_train_bucket)[0]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# The train set encoding, commented for the test set for the sake of allocated memory\n",
    "\n",
    "nr_events_all = []\n",
    "current_online_times = []\n",
    "\n",
    "\n",
    "encoder_config = {'case_id_col': case_id_col,\n",
    "                'static_cat_cols': static_cat_cols,\n",
    "                'static_num_cols': static_num_cols,\n",
    "                'dynamic_cat_cols': dynamic_cat_cols,\n",
    "                'dynamic_num_cols': dynamic_num_cols,\n",
    "                'fillna': True}\n",
    "\n",
    "for bucket in set(test_bucket):\n",
    "    relevant_train_bucket = dm.get_indexes(df_train_prefixes)[bucket == train_bucket]\n",
    "    relevant_test_bucket = dm.get_indexes(df_test_prefixes)[bucket == test_bucket]\n",
    "    \n",
    "    df_test_bucket = dm.get_data_by_indexes(df_test_prefixes, relevant_test_bucket)\n",
    "    test_prfx_len = dm.get_prefix_lengths(df_test_bucket)[0]\n",
    "    test_y = np.array([dm.get_label_numeric(df_test_bucket)])\n",
    "    nr_events_all.extend(list(dm.get_prefix_lengths(df_test_bucket)))\n",
    "    \n",
    "    # checking presence of the train data, if exists, we proceed with the offline training\n",
    "    if len(relevant_train_bucket) == 0:\n",
    "        preds = [dm.get_class_ratio(train)] * len(relevant_test_bucket)\n",
    "        current_online_times.extend([0] * len(preds))\n",
    "    else:\n",
    "        # extracting training data for the experiment\n",
    "        df_train_bucket = dm.get_data_by_indexes(df_train_prefixes, relevant_train_bucket)\n",
    "        train_y_experiment = np.array([dm.get_label_numeric(df_train_bucket)])\n",
    "        prfx_len = dm.get_prefix_lengths(df_train_bucket)[0]\n",
    "        \n",
    "        encoder = get_encoder(encoding, **encoder_config)\n",
    "        featureCombinerExperiment = FeatureUnion(\n",
    "                [(enc_method, encoder) for enc_method in encoding_dict[encoding]])\n",
    "        \n",
    "        encoded_training = featureCombinerExperiment.fit_transform(df_train_bucket)\n",
    "        ffeatures = featureCombinerExperiment.get_feature_names_out()\n",
    "        feat_num = len(ffeatures)\n",
    "        ffeatures.append('encoded_label')\n",
    "\n",
    "        # create a dataframe with the encoded training features and label\n",
    "        encoded_training = np.concatenate((encoded_training,train_y_experiment.T), axis=1)\n",
    "        training_set_df = pd.DataFrame(encoded_training, columns=ffeatures)\n",
    "        bkt_size = training_set_df.shape[0]\n",
    "\n",
    "        # # create a dataframe with the encoded test features and label\n",
    "        # encoded_testing_bucket = featureCombinerExperiment.fit_transform(df_test_bucket)\n",
    "        # encoded_testing_bucket = np.concatenate((encoded_testing_bucket,test_y.T), axis=1)\n",
    "        # testing_set_df = pd.DataFrame(encoded_testing_bucket, columns=ffeatures)\n",
    "        # test_bkt_size = testing_set_df.shape[0]\n",
    "        \n",
    "        # save the preprocessed data and corresponding metadata into a file \n",
    "        outfile_train = 'encoded_training_%s_%s_%s_%s_%s.csv' % (\n",
    "                        dataset_name, method_name, bkt_size, prfx_len, feat_num)\n",
    "        training_set_df.to_csv(os.path.join(encoded_datasets_dir, outfile_train), sep=';', columns=ffeatures, index=False)\n",
    "        print('%s;%s;%s;%s;%s;%s\\n' % (dataset_name, method_name, 'training', bkt_size, prfx_len, feat_num))\n",
    "        \n",
    "        # outfile_test = 'encoded_testing_%s_%s_%s_%s_%s.csv' % (\n",
    "        #                 dataset_name, method_name, test_bkt_size, test_prfx_len, feat_num)\n",
    "        # testing_set_df.to_csv(os.path.join(encoded_datasets_dir, outfile_test), sep=';', columns=ffeatures, index=False)\n",
    "        # print('%s;%s;%s;%s;%s;%s\\n' % (dataset_name, method_name, 'testing', test_bkt_size, test_prfx_len, feat_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
