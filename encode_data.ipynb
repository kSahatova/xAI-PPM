{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from easydict import EasyDict as edict\n",
    "from dataset_manager_optimized import DatasetManager\n",
    "from preprocessing.bucketing import get_bucketer\n",
    "from preprocessing.encoding import get_encoder\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_id_col = \"Case ID\"\n",
    "activity_col = \"Activity\"\n",
    "resource_col = 'org:resource'\n",
    "timestamp_col = 'time:timestamp'\n",
    "label_col = \"label\"\n",
    "pos_label = \"deviant\"\n",
    "neg_label = \"regular\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for classifier\n",
    "# Categorical features \n",
    "dynamic_cat_cols = [activity_col, resource_col, 'Action', 'CreditScore', 'EventOrigin', 'lifecycle:transition',\n",
    "                   \"Accepted\", \"Selected\"] # i.e. event attributes\n",
    "static_cat_cols = ['ApplicationType', 'LoanGoal'] # i.e. case attributes that are known from the start\n",
    "\n",
    "# Numeric features\n",
    "dynamic_num_cols = ['FirstWithdrawalAmount', 'MonthlyCost', 'NumberOfTerms', 'OfferedAmount',\n",
    "                   \"timesincelastevent\", \"timesincecasestart\", \"timesincemidnight\", \"event_nr\", \"month\", \"weekday\", \"hour\",\n",
    "                    \"open_cases\"]\n",
    "static_num_cols = ['RequestedAmount']\n",
    "\n",
    "static_cols = static_cat_cols + static_num_cols + [case_id_col, label_col]\n",
    "dynamic_cols = dynamic_cat_cols + dynamic_num_cols + [timestamp_col]\n",
    "cat_cols = dynamic_cat_cols + static_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = edict({'case_id_col': case_id_col,\n",
    "                           'activity_col': activity_col,\n",
    "                           'resource_col': resource_col,\n",
    "                           'timestamp_col': timestamp_col,\n",
    "                           'label_col': label_col,\n",
    "                           'pos_label_col': pos_label,\n",
    "                           'dynamic_cat_cols': dynamic_cat_cols,\n",
    "                           'static_cat_cols': static_cat_cols,\n",
    "                           'dynamic_num_cols': dynamic_num_cols,\n",
    "                           'static_num_cols': static_num_cols,\n",
    "                'min_prefix_length': 1,\n",
    "                'max_prefix_length': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_column_schema import DatasetSchemas \n",
    "\n",
    "\n",
    "data_dir = r'C:\\Users\\sahat\\OneDrive - KU Leuven\\Research\\PPM&xAI\\data'\n",
    "# '/data/leuven/365/vsc36567/xAI-PPM/data/processed_benchmark_event_logs'\n",
    "ds_name = 'bpic2017' \n",
    "ds_file_names = ['BPIC17_O_Accepted.csv']#, 'BPIC17_O_Cancelled.csv', 'BPIC17_O_Refused.csv']\n",
    "bpic17_column_schema = DatasetSchemas.bpic2017()\n",
    "\n",
    "bucketing_method = 'single'\n",
    "encoding_method = 'agg'\n",
    "encoding_dict = {\n",
    "            \"laststate\": [\"static\", \"last\"],\n",
    "            \"agg\": [\"static\", \"agg\"],\n",
    "            \"index\": [\"static\", \"index\"],\n",
    "            \"combined\": [\"static\", \"last\", \"agg\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the path to your 'processed_benchmark_event_logs' folder \n",
    "# data_dir = r'/data/leuven/365/vsc36567/xAI-PPM/data/processed_benchmark_event_logs' \n",
    "\n",
    "# dataset  #\"BPIC2017_O_Cancelled\", \"BPIC2017_O_Refused\"] were commented to fit the encoded data into available memory \n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2017\" : [\"BPIC17_O_Accepted\"]#, \"BPIC2017_O_Cancelled\", \"BPIC2017_O_Refused\"],\n",
    "}\n",
    "\n",
    "bucketing = 'single'\n",
    "encoding = 'agg'\n",
    "method_name = ('_').join([bucketing, encoding])\n",
    "\n",
    "encoding_dict = {\n",
    "    'agg' : ['static', 'agg'],\n",
    "    'index' : ['static', 'index']\n",
    "    }\n",
    "\n",
    "gap = 1\n",
    "train_ratio = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "import time \n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train set:  (927785, 26) \n",
      "Shape of the test set:  (239791, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating prefixes: 100%|██████████| 19/19 [00:06<00:00,  2.96it/s]\n",
      "Generating prefixes: 100%|██████████| 19/19 [00:01<00:00, 13.02it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'encoded_training' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 66\u001b[0m\n\u001b[0;32m     61\u001b[0m         enc_fnames\u001b[38;5;241m.\u001b[39mappend(new_fname)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# enc_fnames.append('encoded_label')\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# create a dataframe with the encoded training features and label\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# encoded_training = np.concatenate((encoded_training, train_y.reshape(-1, 1)), axis=1)\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m training_set_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mencoded_training\u001b[49m, columns\u001b[38;5;241m=\u001b[39menc_fnames)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoded_training' is not defined"
     ]
    }
   ],
   "source": [
    "# Read the datasets\n",
    "# DataManager splits the data and extracts prefixes \n",
    "\n",
    "dataset_name = 'bpic2017'\n",
    "train_prefixes = {}\n",
    "test_prefixes = {}\n",
    "\n",
    "encoder_config = bpic17_column_schema.get_encoder_args(fillna=True)\n",
    "# encoder_config = {'case_id_col': case_id_col,\n",
    "#                 'static_cat_cols': static_cat_cols,\n",
    "#                 'static_num_cols': static_num_cols,\n",
    "#                 'dynamic_cat_cols': dynamic_cat_cols,\n",
    "#                 'dynamic_num_cols': dynamic_num_cols,\n",
    "#                 'fillna': True}\n",
    "\n",
    "start_time = time.time()\n",
    "for file_name in dataset_ref_to_datasets[dataset_name]:\n",
    "    activity = file_name.split('_')[-1]\n",
    "    dm = DatasetManager(dataset_name, bpic17_column_schema)\n",
    "    df = dm.read_dataset(osp.join(data_dir, file_name + '.csv'))\n",
    "    min_prefix_length = 1\n",
    "    max_prefix_length = min(config.max_prefix_length, dm.get_pos_case_length_quantile(df, 0.90))\n",
    "\n",
    "    # Splitting the data into train and test set \n",
    "    train, test = dm.split_data_strict(df, train_ratio=train_ratio, split='temporal')\n",
    "    print('Shape of the train set: ', train.shape, '\\nShape of the test set: ', test.shape)\n",
    "    \n",
    "    # Generating prefixes \n",
    "    df_train_prefixes = dm.generate_prefix_data(train, min_prefix_length, max_prefix_length)\n",
    "    df_test_prefixes = dm.generate_prefix_data(test, min_prefix_length, max_prefix_length)\n",
    "    \n",
    "    # Create buckets for each prexif or a single one that fits all the prexifes  \n",
    "    bucketer = get_bucketer(method=bucketing, case_id_col=case_id_col)\n",
    "    train_bucket = bucketer.fit_predict(df_train_prefixes)\n",
    "    test_bucket = bucketer.predict(df_test_prefixes)\n",
    "\n",
    "    # Iterating over the set of generated buckets\n",
    "    for bucket in set(test_bucket):\n",
    "\n",
    "        train_bucket_ind = dm.get_indexes(df_train_prefixes)[bucket == train_bucket]\n",
    "        test_bucket_ind = dm.get_indexes(df_test_prefixes)[bucket == test_bucket]   \n",
    "\n",
    "        # extracting training data for the experiment\n",
    "        df_train_bucket = dm.get_data_by_indexes(df_train_prefixes, train_bucket_ind)\n",
    "        df_test_bucket = dm.get_data_by_indexes(df_test_prefixes, test_bucket_ind)\n",
    "        \n",
    "        _, train_y = np.asarray(dm.get_labels(df_train_bucket))\n",
    "        _, test_y = np.asarray(dm.get_labels(df_test_bucket))\n",
    "\n",
    "        # Get a set of encoders for preprocessing of static and dynamic features\n",
    "        featureCombinerExperiment = FeatureUnion(\n",
    "                [(enc_method, get_encoder(enc_method, **encoder_config)) for enc_method in encoding_dict[encoding]])\n",
    "        \n",
    "        encoded_train_bucket = featureCombinerExperiment.fit_transform(df_train_bucket)\n",
    "        encoded_test_bucket = featureCombinerExperiment.fit_transform(df_test_bucket)        \n",
    "\n",
    "\n",
    "        enc_fnames = []\n",
    "        for _, transformer in featureCombinerExperiment.transformer_list:\n",
    "            for new_fname in transformer.get_feature_names():\n",
    "                enc_fnames.append(new_fname)\n",
    "        # enc_fnames.append('encoded_label')\n",
    "\n",
    "        # create a dataframe with the encoded training features and label\n",
    "        # encoded_training = np.concatenate((encoded_training, train_y.reshape(-1, 1)), axis=1)\n",
    "        training_set_df = pd.DataFrame(encoded_training, columns=enc_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((927785, 26), (5119334, 29), (469762, 29))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, df_train_prefixes.shape, df_train_bucket.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
